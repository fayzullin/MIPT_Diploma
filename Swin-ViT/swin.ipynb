{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='./data'\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "test_dset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_show_image(dset, idx):\n",
    "    X, Y = dset[idx]\n",
    "    title = \"Ground truth: {}\".format(dset.classes[Y])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(np.moveaxis(X.numpy(), 0, -1))\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_show_image(test_dset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                          num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(*layers)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.gamma * self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowAttention(nn.Module):\n",
    "    def __init__(self, dim, head_dim, shape, window_size, shift_size=0):\n",
    "        super().__init__()\n",
    "        self.heads = dim // head_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = head_dim**-0.5\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.unifyheads = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.pos_enc = nn.Parameter(torch.Tensor(self.heads, (2 * window_size - 1)**2))\n",
    "        self.register_buffer(\"relative_indices\", self.get_indices(window_size))\n",
    "        \n",
    "        if shift_size > 0:\n",
    "            self.register_buffer(\"mask\", self.generate_mask(shape, window_size, shift_size))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        shift_size, window_size = self.shift_size, self.window_size\n",
    "        \n",
    "        x = self.to_windows(x, self.shape, window_size, shift_size) # partition into windows\n",
    "        \n",
    "        # self attention\n",
    "        qkv = self.to_qkv(x).unflatten(-1, (3, self.heads, self.head_dim)).transpose(-2, 1)\n",
    "        queries, keys, values = qkv.unbind(dim=2)\n",
    "        \n",
    "        att = queries @ keys.transpose(-2, -1)\n",
    "        \n",
    "        att = att * self.scale + self.get_rel_pos_enc(window_size) # add relative positon encoding\n",
    "        \n",
    "        # masking\n",
    "        if shift_size > 0:\n",
    "            att = self.mask_attention(att)\n",
    "        \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        x = att @ values\n",
    "        x = x.transpose(1, 2).contiguous().flatten(-2, -1) # move head back\n",
    "        x = self.unifyheads(x)\n",
    "        \n",
    "        x = self.from_windows(x, self.shape, window_size, shift_size) # undo partitioning into windows\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def to_windows(self, x, shape, window_size, shift_size):\n",
    "        x = x.unflatten(1, shape)\n",
    "        if shift_size > 0:\n",
    "            x = x.roll((-shift_size, -shift_size), dims=(1, 2))\n",
    "        x = self.split_windows(x, window_size)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def from_windows(self, x, shape, window_size, shift_size):\n",
    "        x = self.merge_windows(x, shape, window_size) \n",
    "        if shift_size > 0:\n",
    "            x = x.roll((shift_size, shift_size), dims=(1, 2))\n",
    "        x = x.flatten(1, 2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def mask_attention(self, att):\n",
    "        num_win = self.mask.size(1)\n",
    "        att = att.unflatten(0, (att.size(0) // num_win, num_win))\n",
    "        att = att.masked_fill(self.mask, float('-inf'))\n",
    "        att = att.flatten(0, 1)\n",
    "        return att\n",
    "    \n",
    "    \n",
    "    def get_rel_pos_enc(self, window_size):\n",
    "        indices = self.relative_indices.expand(self.heads, -1)\n",
    "        rel_pos_enc = self.pos_enc.gather(-1, indices)\n",
    "        rel_pos_enc = rel_pos_enc.unflatten(-1, (window_size**2, window_size**2))\n",
    "        return rel_pos_enc\n",
    "    \n",
    "    \n",
    "    # For explanation of mask regions see Figure 4 in the article\n",
    "    @staticmethod\n",
    "    def generate_mask(shape, window_size, shift_size):\n",
    "        region_mask = torch.zeros(1, *shape, 1)\n",
    "        slices = [slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None)]\n",
    "        \n",
    "        region_num = 0\n",
    "        for i in slices:\n",
    "            for j in slices:\n",
    "                region_mask[:, i, j, :] = region_num\n",
    "                region_num += 1\n",
    "\n",
    "        mask_windows = ShiftedWindowAttention.split_windows(region_mask, window_size).squeeze(-1)\n",
    "        diff_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        mask = diff_mask != 0\n",
    "        mask = mask.unsqueeze(1).unsqueeze(0) # add heads and batch dimension\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def split_windows(x, window_size):\n",
    "        n_h, n_w = x.size(1) // window_size, x.size(2) // window_size\n",
    "        x = x.unflatten(1, (n_h, window_size)).unflatten(-2, (n_w, window_size)) # split into windows\n",
    "        x = x.transpose(2, 3).flatten(0, 2) # merge batch and window numbers\n",
    "        x = x.flatten(-3, -2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_windows(x, shape, window_size):\n",
    "        n_h, n_w = shape[0] // window_size, shape[1] // window_size\n",
    "        b = x.size(0) // (n_h * n_w)\n",
    "        x = x.unflatten(1, (window_size, window_size))\n",
    "        x = x.unflatten(0, (b, n_h, n_w)).transpose(2, 3) # separate batch and window numbers\n",
    "        x = x.flatten(1, 2).flatten(-3, -2) # merge windows\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_indices(window_size):\n",
    "        x = torch.arange(window_size, dtype=torch.long)\n",
    "        \n",
    "        y1, x1, y2, x2 = torch.meshgrid(x, x, x, x, indexing='ij')\n",
    "        indices = (y1 - y2 + window_size - 1) * (2 * window_size - 1) + x1 - x2 + window_size - 1\n",
    "        indices = indices.flatten()\n",
    "        \n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        hidden_dim = dim * mult\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Sequential):\n",
    "    def __init__(self, dim, head_dim, shape, window_size, shift_size=0, p_drop=0.):\n",
    "        super().__init__(\n",
    "            Residual(\n",
    "                nn.LayerNorm(dim),\n",
    "                ShiftedWindowAttention(dim, head_dim, shape, window_size, shift_size),\n",
    "                nn.Dropout(p_drop)\n",
    "            ),\n",
    "            Residual(\n",
    "                nn.LayerNorm(dim),\n",
    "                FeedForward(dim),\n",
    "                nn.Dropout(p_drop)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.norm = nn.LayerNorm(4 * in_dim)\n",
    "        self.reduction = nn.Linear(4 * in_dim, out_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unflatten(1, self.shape).movedim(-1, 1)\n",
    "        x = F.unfold(x, kernel_size=2, stride=2).movedim(1, -1)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, num_blocks, in_dim, out_dim, head_dim, shape, window_size, p_drop=0.):\n",
    "        if out_dim != in_dim:\n",
    "            layers = [PatchMerging(in_dim, out_dim, shape)]\n",
    "            shape = (shape[0] // 2, shape[1] // 2)\n",
    "        else:\n",
    "            layers = []\n",
    "        \n",
    "        shift_size = window_size // 2\n",
    "        layers += [TransformerBlock(out_dim, head_dim, shape, window_size, 0 if (num % 2 == 0) else shift_size,\n",
    "                                    p_drop) for num in range(num_blocks)]\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageStack(nn.Sequential):\n",
    "    def __init__(self, num_blocks_list, dims, head_dim, shape, window_size, p_drop=0.):\n",
    "        layers = []\n",
    "        in_dim = dims[0]\n",
    "        for num, out_dim in zip(num_blocks_list, dims[1:]):\n",
    "            layers.append(Stage(num, in_dim, out_dim, head_dim, shape, window_size, p_drop))\n",
    "            if in_dim != out_dim:\n",
    "                shape = (shape[0] // 2, shape[1] // 2)\n",
    "                in_dim = out_dim\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToPatches(nn.Module):\n",
    "    def __init__(self, in_channels, dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        patch_dim = in_channels * patch_size**2\n",
    "        self.proj = nn.Linear(patch_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.unfold(x, kernel_size=self.patch_size, stride=self.patch_size).movedim(1, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_patches):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.Tensor(num_patches, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToEmbedding(nn.Sequential):\n",
    "    def __init__(self, in_channels, dim, patch_size, num_patches, p_drop=0.):\n",
    "        super().__init__(\n",
    "            ToPatches(in_channels, dim, patch_size),\n",
    "            AddPositionEmbedding(dim, num_patches),\n",
    "            nn.Dropout(p_drop)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Sequential):\n",
    "    def __init__(self, dim, classes, p_drop=0.):\n",
    "        super().__init__(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            GlobalAvgPool(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(dim, classes)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Sequential):\n",
    "    def __init__(self, classes, image_size, num_blocks_list, dims, head_dim, patch_size, window_size,\n",
    "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
    "        reduced_size = image_size // patch_size\n",
    "        shape = (reduced_size, reduced_size)\n",
    "        num_patches = shape[0] * shape[1]\n",
    "        \n",
    "        super().__init__(\n",
    "            ToEmbedding(in_channels, dims[0], patch_size, num_patches, emb_p_drop),\n",
    "            StageStack(num_blocks_list, dims, head_dim, shape, window_size, trans_p_drop),\n",
    "            Head(dims[-1], classes, head_p_drop)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, AddPositionEmbedding):\n",
    "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, ShiftedWindowAttention):\n",
    "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, Residual):\n",
    "                nn.init.zeros_(m.gamma)\n",
    "    \n",
    "    def separate_parameters(self):\n",
    "        parameters_decay = set()\n",
    "        parameters_no_decay = set()\n",
    "        modules_weight_decay = (nn.Linear, )\n",
    "        modules_no_weight_decay = (nn.LayerNorm,)\n",
    "\n",
    "        for m_name, m in self.named_modules():\n",
    "            for param_name, param in m.named_parameters():\n",
    "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "                if isinstance(m, modules_no_weight_decay):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif param_name.endswith(\"bias\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, ShiftedWindowAttention) and param_name.endswith(\"pos_enc\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, modules_weight_decay):\n",
    "                    parameters_decay.add(full_param_name)\n",
    "\n",
    "        # sanity check\n",
    "        assert len(parameters_decay & parameters_no_decay) == 0\n",
    "        assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "        return parameters_decay, parameters_no_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinTransformer(NUM_CLASSES, IMAGE_SIZE,\n",
    "                        num_blocks_list=[4, 4], dims=[128, 128, 256],\n",
    "                        head_dim=32, patch_size=2, window_size=4,\n",
    "                        emb_p_drop=0., trans_p_drop=0., head_p_drop=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4,124,362\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate, weight_decay):\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    parameters_decay, parameters_no_decay = model.separate_parameters()\n",
    "    \n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in parameters_decay], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [param_dict[pn] for pn in parameters_no_decay], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optim_groups, lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, loss, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
    "                                             steps_per_epoch=len(train_loader), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.add_event_handler(Events.ITERATION_COMPLETED, lambda engine: lr_scheduler.step());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ignite.metrics.RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_metrics = {\"accuracy\": ignite.metrics.Accuracy(), \"loss\": ignite.metrics.Loss(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    train_state = engine.state\n",
    "    epoch = train_state.epoch\n",
    "    max_epochs = train_state.max_epochs\n",
    "    train_loss = train_state.metrics[\"loss\"]\n",
    "    history['train loss'].append(train_loss)\n",
    "    \n",
    "    evaluator.run(test_loader)\n",
    "    val_metrics = evaluator.state.metrics\n",
    "    val_loss = val_metrics[\"loss\"]\n",
    "    val_acc = val_metrics[\"accuracy\"]\n",
    "    history['val loss'].append(val_loss)\n",
    "    history['val acc'].append(val_acc)\n",
    "    \n",
    "    print(\"{}/{} - train: loss {:.3f}; val: loss {:.3f} accuracy {:.3f}\".format(\n",
    "        epoch, max_epochs, train_loss, val_loss, val_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 - train: loss 1.918; val: loss 1.849 accuracy 0.305\n",
      "2/100 - train: loss 1.699; val: loss 1.640 accuracy 0.389\n",
      "3/100 - train: loss 1.506; val: loss 1.427 accuracy 0.470\n",
      "4/100 - train: loss 1.405; val: loss 1.362 accuracy 0.514\n",
      "5/100 - train: loss 1.303; val: loss 1.291 accuracy 0.536\n",
      "6/100 - train: loss 1.279; val: loss 1.260 accuracy 0.542\n",
      "7/100 - train: loss 1.181; val: loss 1.136 accuracy 0.591\n",
      "8/100 - train: loss 1.165; val: loss 1.106 accuracy 0.603\n",
      "9/100 - train: loss 1.123; val: loss 1.067 accuracy 0.621\n",
      "10/100 - train: loss 1.094; val: loss 1.017 accuracy 0.633\n",
      "11/100 - train: loss 1.042; val: loss 0.998 accuracy 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7aa3a42c3bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterrupt_resume_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run_legacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run_as_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    957\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                     \u001b[0mepoch_time_taken\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset_as_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                     \u001b[0;31m# time is available for handlers but must be updated after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_terminate_or_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_terminate_or_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mdevice_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mdevice_exp_avgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         device_exp_avg_sqs = [\n\u001b[1;32m    488\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mdevice_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mdevice_exp_avgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         device_exp_avg_sqs = [\n\u001b[1;32m    488\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=EPOCHS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "xs = np.arange(1, len(history['train loss']) + 1)\n",
    "ax.plot(xs, history['train loss'], '.-', label='train')\n",
    "ax.plot(xs, history['val loss'], '.-', label='val')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "xs = np.arange(1, len(history['val acc']) + 1)\n",
    "ax.plot(xs, history['val acc'], '-')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('val acc')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './models/swin_weights.pkl'\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
